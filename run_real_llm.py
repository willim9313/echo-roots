#!/usr/bin/env python3
"""
å¯¦éš›ä½¿ç”¨æ‚¨çš„ LLM API é‹è¡Œ Echo-Roots æå–çš„è…³æœ¬
Real LLM API integration script for Echo-Roots
"""

import asyncio
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
import aiohttp

# æ·»åŠ  src åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent / "src"))

try:
    from echo_roots.models.domain import DomainPack
    from echo_roots.models.core import IngestionItem, AttributeExtraction, SemanticTerm, ExtractionResult, ExtractionMetadata
    from echo_roots.storage.duckdb_backend import DuckDBStorageManager
    from echo_roots.domain.adapter import DomainAdapter
except ImportError as e:
    print(f"âŒ ç„¡æ³•å°å…¥ Echo-Roots æ¨¡çµ„: {e}")
    print("è«‹ç¢ºä¿æ‚¨åœ¨æ­£ç¢ºçš„ç›®éŒ„ä¸­é‹è¡Œæ­¤è…³æœ¬")
    sys.exit(1)


class GeminiLLMClient:
    """Google Gemini API å®¢æˆ¶ç«¯"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-1.5-flash", **kwargs):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = 'https://generativelanguage.googleapis.com/v1beta'
        self.session = None
    
    async def complete(self, prompt: str, **kwargs) -> str:
        """ç™¼é€è«‹æ±‚åˆ° Gemini API"""
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        # Gemini API ä½¿ç”¨ URL åƒæ•¸å‚³é API key
        url = f'{self.base_url}/models/{self.model_name}:generateContent?key={self.api_key}'
        
        headers = {
            'Content-Type': 'application/json'
        }
        
        # Gemini API çš„è«‹æ±‚æ ¼å¼
        payload = {
            "contents": [
                {
                    "parts": [
                        {
                            "text": f"You are a product data extraction expert. Return valid JSON only.\n\n{prompt}"
                        }
                    ]
                }
            ],
            "generationConfig": {
                "temperature": kwargs.get('temperature', 0.3),
                "topK": 40,
                "topP": 0.95,
                "maxOutputTokens": kwargs.get('max_tokens', 2000),
                "stopSequences": []
            }
        }
        
        try:
            async with self.session.post(
                url,
                headers=headers,
                json=payload,
                timeout=30
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # è§£æ Gemini çš„å›æ‡‰æ ¼å¼
                    if 'candidates' in data and len(data['candidates']) > 0:
                        candidate = data['candidates'][0]
                        if 'content' in candidate and 'parts' in candidate['content']:
                            parts = candidate['content']['parts']
                            if len(parts) > 0 and 'text' in parts[0]:
                                return parts[0]['text'].strip()
                    
                    raise Exception("Invalid response format from Gemini API")
                else:
                    error_text = await response.text()
                    raise Exception(f"Gemini API Error {response.status}: {error_text}")
                    
        except Exception as e:
            raise Exception(f"Gemini API call failed: {str(e)}")
    
    async def close(self):
        if self.session:
            await self.session.close()


class OpenAILLMClient:
    """OpenAI API å®¢æˆ¶ç«¯"""
    
    def __init__(self, api_key: str, model_name: str = "gpt-4", **kwargs):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = kwargs.get('base_url', 'https://api.openai.com/v1')
        self.temperature = kwargs.get('temperature', 0.3)
        self.max_tokens = kwargs.get('max_tokens', 2000)
        self.session = None
    
    async def complete(self, prompt: str, **kwargs) -> str:
        """ç™¼é€è«‹æ±‚åˆ° OpenAI API"""
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'model': self.model_name,
            'messages': [
                {
                    'role': 'system', 
                    'content': 'You are a product data extraction expert. Return valid JSON only.'
                },
                {
                    'role': 'user', 
                    'content': prompt
                }
            ],
            'temperature': kwargs.get('temperature', self.temperature),
            'max_tokens': kwargs.get('max_tokens', self.max_tokens)
        }
        
        try:
            async with self.session.post(
                f'{self.base_url}/chat/completions',
                headers=headers,
                json=payload,
                timeout=30
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return data['choices'][0]['message']['content'].strip()
                else:
                    error_text = await response.text()
                    raise Exception(f"OpenAI API Error {response.status}: {error_text}")
                    
        except Exception as e:
            raise Exception(f"OpenAI API call failed: {str(e)}")
    
    async def close(self):
        """é—œé–‰æœƒè©±"""
        if self.session:
            await self.session.close()


class RealLLMExtractor:
    """ä½¿ç”¨çœŸå¯¦ LLM API çš„æå–å™¨"""
    
    def __init__(self, domain_pack: DomainPack, llm_client):
        self.domain_pack = domain_pack
        self.llm_client = llm_client
        
    def build_extraction_prompt(self, item: IngestionItem) -> str:
        """æ§‹å»ºæå–æç¤º"""
        # ä½¿ç”¨åŸŸåŒ…ä¸­çš„æç¤ºæ¨¡æ¿
        extraction_prompt = self.domain_pack.llm_prompts.get('extraction', '')
        category_prompt = self.domain_pack.llm_prompts.get('category_classification', '')
        
        # æ§‹å»ºå®Œæ•´æç¤º
        prompt = f"""
{extraction_prompt}

Product Information:
Title: {item.title}
Description: {item.description}
Content: {item.raw_content}

è«‹ä»¥ä»¥ä¸‹ JSON æ ¼å¼å›æ‡‰:
{{
    "attributes": [
        {{"name": "attribute_name", "value": "attribute_value", "confidence": 0.95}}
    ],
    "terms": [
        {{"term": "technical_term", "confidence": 0.90}}
    ],
    "category": "predicted_category"
}}

{category_prompt}
"""
        return prompt
    
    async def extract_single(self, item: IngestionItem) -> ExtractionResult:
        """å°å–®ä¸€é …ç›®é€²è¡Œæå–"""
        import time
        start_time = time.time()
        
        try:
            # æ§‹å»ºæç¤º
            prompt = self.build_extraction_prompt(item)
            
            # èª¿ç”¨ LLM
            response = await self.llm_client.complete(prompt)
            
            # è§£æå›æ‡‰
            try:
                result_data = json.loads(response)
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆ JSONï¼Œå˜—è©¦æå– JSON éƒ¨åˆ†
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    result_data = json.loads(json_match.group())
                else:
                    raise Exception("ç„¡æ³•å¾ LLM å›æ‡‰ä¸­è§£æ JSON")
            
            # å‰µå»ºå±¬æ€§æå–çµæœ
            attributes = []
            for attr_data in result_data.get('attributes', []):
                attributes.append(AttributeExtraction(
                    name=attr_data['name'],
                    value=attr_data['value'],
                    confidence=attr_data.get('confidence', 0.8)
                ))
            
            # å‰µå»ºè¡“èªæå–çµæœ
            terms = []
            for term_data in result_data.get('terms', []):
                terms.append(SemanticTerm(
                    term=term_data['term'],
                    confidence=term_data.get('confidence', 0.8)
                ))
            
            # å‰µå»ºå…ƒæ•¸æ“š
            processing_time = int((time.time() - start_time) * 1000)
            metadata = ExtractionMetadata(
                model=self.llm_client.model_name,
                run_id=f"run_{int(time.time())}",
                extracted_at=datetime.now(),
                processing_time_ms=processing_time
            )
            
            return ExtractionResult(
                item_id=item.id,
                attributes=attributes,
                terms=terms,
                predicted_category=result_data.get('category', ''),
                metadata=metadata
            )
            
        except Exception as e:
            # å‰µå»ºå¤±æ•—çµæœ
            processing_time = int((time.time() - start_time) * 1000)
            metadata = ExtractionMetadata(
                model=self.llm_client.model_name,
                run_id=f"run_{int(time.time())}",
                extracted_at=datetime.now(),
                processing_time_ms=processing_time
            )
            
            return ExtractionResult(
                item_id=item.id,
                attributes=[],
                terms=[],
                predicted_category='',
                metadata=metadata
            )


def load_config() -> Dict[str, Any]:
    """è¼‰å…¥é…ç½®"""
    provider = os.getenv('LLM_PROVIDER', 'gemini')  # é è¨­ä½¿ç”¨ Gemini
    
    if provider == 'gemini':
        config = {
            'provider': 'gemini',
            'api_key': os.getenv('GOOGLE_API_KEY', ''),
            'model_name': os.getenv('GEMINI_MODEL', 'gemini-1.5-flash'),
            'temperature': 0.3,
            'max_tokens': 2000
        }
        
        if not config['api_key']:
            print("âŒ è«‹è¨­å®š GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸")
            print("æ‚¨å¯ä»¥:")
            print("1. å‰å¾€ https://aistudio.google.com/app/apikey ç²å– API é‡‘é‘°")
            print("2. åŸ·è¡Œ: export GOOGLE_API_KEY='your-gemini-api-key'")
            print("3. æˆ–åœ¨ .env æ–‡ä»¶ä¸­è¨­å®š GOOGLE_API_KEY")
            return None
            
    elif provider == 'openai':
        config = {
            'provider': 'openai',
            'api_key': os.getenv('OPENAI_API_KEY', ''),
            'model_name': os.getenv('OPENAI_MODEL', 'gpt-4'),
            'temperature': 0.3,
            'max_tokens': int(os.getenv('OPENAI_MAX_TOKENS', '2000'))
        }
        
        if not config['api_key']:
            print("âŒ è«‹è¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
            return None
    else:
        print(f"âŒ ä¸æ”¯æ´çš„ LLM æä¾›å•†: {provider}")
        return None
    
    return config


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¤– Echo-Roots çœŸå¯¦ LLM API æ•´åˆæ¸¬è©¦")
    print("=" * 50)
    
    # è¼‰å…¥é…ç½®
    config = load_config()
    if not config:
        return
    
    # å‰µå»º LLM å®¢æˆ¶ç«¯
    print(f"ğŸ”— é€£æ¥åˆ° {config['provider']} {config['model_name']}...")
    
    if config['provider'] == 'gemini':
        llm_client = GeminiLLMClient(
            api_key=config['api_key'],
            model_name=config['model_name']
        )
    elif config['provider'] == 'openai':
        llm_client = OpenAILLMClient(
            api_key=config['api_key'],
            model_name=config['model_name'],
            temperature=config['temperature'],
            max_tokens=config['max_tokens']
        )
    else:
        print(f"âŒ ä¸æ”¯æ´çš„ LLM æä¾›å•†: {config['provider']}")
        return
    
    try:
        # è¼‰å…¥é ˜åŸŸåŒ…
        print("ğŸ“¦ è¼‰å…¥é›»å•†é ˜åŸŸåŒ…...")
        domain_path = Path("domains/ecommerce/domain.yaml")
        if not domain_path.exists():
            print(f"âŒ æ‰¾ä¸åˆ°é ˜åŸŸåŒ…æ–‡ä»¶: {domain_path}")
            return
        
        domain_adapter = DomainAdapter.from_file(domain_path)
        domain_pack = domain_adapter.domain_pack
        
        # åˆå§‹åŒ–å­˜å„²
        print("ğŸ’¾ é€£æ¥åˆ°è³‡æ–™åº«...")
        storage_config = {
            "duckdb": {
                "database_path": "./test_echo_roots.db"
            }
        }
        
        storage_manager = DuckDBStorageManager(storage_config)
        await storage_manager.initialize()
        
        # å‰µå»ºæå–å™¨
        extractor = RealLLMExtractor(domain_pack, llm_client)
        
        # ç²å–æ¸¬è©¦é …ç›®
        print("ğŸ“‹ ç²å–æ¸¬è©¦è³‡æ–™...")
        items = await storage_manager.ingestion.list_items(limit=3)
        
        if not items:
            print("âŒ æ²’æœ‰æ‰¾åˆ°æ¸¬è©¦è³‡æ–™ã€‚è«‹å…ˆé‹è¡Œ test_data_processing.py")
            return
        
        print(f"âœ… æ‰¾åˆ° {len(items)} å€‹æ¸¬è©¦é …ç›®")
        
        # é‹è¡Œæå–
        print(f"\nğŸ”„ é–‹å§‹ä½¿ç”¨ {config['model_name']} é€²è¡Œæå–...")
        
        total_attributes = 0
        total_terms = 0
        total_time = 0
        
        for i, item in enumerate(items, 1):
            print(f"\nğŸ“ è™•ç†é …ç›® {i}: {item.title}")
            
            try:
                result = await extractor.extract_single(item)
                
                if result.metadata.error_message:
                    print(f"âŒ æå–å¤±æ•—: {result.metadata.error_message}")
                    continue
                
                # é¡¯ç¤ºçµæœ
                print(f"âœ… æå–äº† {len(result.attributes)} å€‹å±¬æ€§:")
                for attr in result.attributes:
                    print(f"  - {attr.name}: {attr.value} (ä¿¡å¿ƒåº¦: {attr.confidence:.2f})")
                    total_attributes += 1
                
                print(f"âœ… æå–äº† {len(result.terms)} å€‹è¡“èª:")
                for term in result.terms[:3]:  # åªé¡¯ç¤ºå‰ 3 å€‹
                    print(f"  - {term.term} (ä¿¡å¿ƒåº¦: {term.confidence:.2f})")
                    total_terms += 1
                
                if result.predicted_category:
                    print(f"ğŸ·ï¸ é æ¸¬é¡åˆ¥: {result.predicted_category}")
                
                print(f"â±ï¸ è™•ç†æ™‚é–“: {result.metadata.processing_time_ms}ms")
                total_time += result.metadata.processing_time_ms
                
                # ä¿å­˜çµæœåˆ°è³‡æ–™åº«ï¼ˆå¯é¸ï¼‰
                # await storage_manager.extraction.save_result(result)
                
            except Exception as e:
                print(f"âŒ è™•ç†é …ç›®æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        
        # é¡¯ç¤ºçµ±è¨ˆ
        print(f"\nğŸ“Š æå–çµ±è¨ˆ:")
        print(f"  - ç¸½å±¬æ€§æ•¸: {total_attributes}")
        print(f"  - ç¸½è¡“èªæ•¸: {total_terms}")
        print(f"  - å¹³å‡è™•ç†æ™‚é–“: {total_time / len(items):.0f}ms")
        print(f"  - ä½¿ç”¨æ¨¡å‹: {config['model_name']}")
        
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        
    finally:
        # æ¸…ç†è³‡æº
        print("\nğŸ§¹ æ¸…ç†è³‡æº...")
        await llm_client.close()
        await storage_manager.close()
        
    print("\nğŸ‰ LLM æ•´åˆæ¸¬è©¦å®Œæˆï¼")


if __name__ == "__main__":
    # æª¢æŸ¥ä¾è³´
    try:
        import aiohttp
    except ImportError:
        print("âŒ éœ€è¦å®‰è£ aiohttp: pip install aiohttp")
        sys.exit(1)
    
    asyncio.run(main())
