#!/usr/bin/env python3
"""
LLM API æ•´åˆç¯„ä¾‹
Examples of how to integrate your own LLM APIs with Echo-Roots
"""

import asyncio
import json
import logging
from typing import Dict, Any, Optional
from abc import ABC, abstractmethod

# ä¸åŒ LLM æä¾›å•†çš„å®¢æˆ¶ç«¯
import openai  # pip install openai
import anthropic  # pip install anthropic
import requests  # è‡ªè¨‚ API


class BaseLLMClient(ABC):
    """LLM å®¢æˆ¶ç«¯åŸºç¤é¡åˆ¥"""
    
    def __init__(self, api_key: str, model_name: str, **kwargs):
        self.api_key = api_key
        self.model_name = model_name
        self.logger = logging.getLogger(self.__class__.__name__)
    
    @abstractmethod
    async def complete(self, prompt: str, **kwargs) -> str:
        """å®Œæˆæç¤ºçš„æŠ½è±¡æ–¹æ³•"""
        pass


class OpenAIClient(BaseLLMClient):
    """OpenAI GPT å®¢æˆ¶ç«¯"""
    
    def __init__(self, api_key: str, model_name: str = "gpt-4", **kwargs):
        super().__init__(api_key, model_name, **kwargs)
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.temperature = kwargs.get('temperature', 0.3)
        self.max_tokens = kwargs.get('max_tokens', 2000)
    
    async def complete(self, prompt: str, **kwargs) -> str:
        """ä½¿ç”¨ OpenAI API å®Œæˆæç¤º"""
        try:
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a product data extraction expert. Return valid JSON only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=kwargs.get('temperature', self.temperature),
                max_tokens=kwargs.get('max_tokens', self.max_tokens),
                response_format={"type": "json_object"}  # ç¢ºä¿è¿”å› JSON
            )
            
            content = response.choices[0].message.content.strip()
            self.logger.info(f"OpenAI response received, length: {len(content)}")
            return content
            
        except Exception as e:
            self.logger.error(f"OpenAI API error: {str(e)}")
            raise


class AnthropicClient(BaseLLMClient):
    """Anthropic Claude å®¢æˆ¶ç«¯"""
    
    def __init__(self, api_key: str, model_name: str = "claude-3-sonnet-20240229", **kwargs):
        super().__init__(api_key, model_name, **kwargs)
        self.client = anthropic.AsyncAnthropic(api_key=api_key)
        self.max_tokens = kwargs.get('max_tokens', 2000)
    
    async def complete(self, prompt: str, **kwargs) -> str:
        """ä½¿ç”¨ Anthropic API å®Œæˆæç¤º"""
        try:
            response = await self.client.messages.create(
                model=self.model_name,
                max_tokens=kwargs.get('max_tokens', self.max_tokens),
                messages=[
                    {
                        "role": "user", 
                        "content": f"{prompt}\n\nPlease respond with valid JSON only."
                    }
                ]
            )
            
            content = response.content[0].text.strip()
            self.logger.info(f"Anthropic response received, length: {len(content)}")
            return content
            
        except Exception as e:
            self.logger.error(f"Anthropic API error: {str(e)}")
            raise


class CustomAPIClient(BaseLLMClient):
    """è‡ªè¨‚ API å®¢æˆ¶ç«¯ç¯„ä¾‹"""
    
    def __init__(self, api_key: str, base_url: str, model_name: str, **kwargs):
        super().__init__(api_key, model_name, **kwargs)
        self.base_url = base_url.rstrip('/')
        self.timeout = kwargs.get('timeout', 30)
        self.headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
    
    async def complete(self, prompt: str, **kwargs) -> str:
        """ä½¿ç”¨è‡ªè¨‚ API å®Œæˆæç¤º"""
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "max_tokens": kwargs.get('max_tokens', 2000),
                "temperature": kwargs.get('temperature', 0.3),
                "format": "json"
            }
            
            # ä½¿ç”¨ requests (åŒæ­¥) æˆ– aiohttp (ç•°æ­¥)
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/v1/completions",
                    headers=self.headers,
                    json=payload,
                    timeout=self.timeout
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data.get('choices', [{}])[0].get('text', '').strip()
                        self.logger.info(f"Custom API response received, length: {len(content)}")
                        return content
                    else:
                        raise Exception(f"API returned status {response.status}: {await response.text()}")
                        
        except Exception as e:
            self.logger.error(f"Custom API error: {str(e)}")
            raise


class AzureOpenAIClient(BaseLLMClient):
    """Azure OpenAI å®¢æˆ¶ç«¯"""
    
    def __init__(self, api_key: str, endpoint: str, deployment_name: str, api_version: str = "2024-02-15-preview", **kwargs):
        super().__init__(api_key, deployment_name, **kwargs)
        self.client = openai.AsyncAzureOpenAI(
            api_key=api_key,
            api_version=api_version,
            azure_endpoint=endpoint
        )
        self.deployment_name = deployment_name
        self.temperature = kwargs.get('temperature', 0.3)
        self.max_tokens = kwargs.get('max_tokens', 2000)
    
    async def complete(self, prompt: str, **kwargs) -> str:
        """ä½¿ç”¨ Azure OpenAI API å®Œæˆæç¤º"""
        try:
            response = await self.client.chat.completions.create(
                model=self.deployment_name,  # Azure ä½¿ç”¨ deployment name
                messages=[
                    {"role": "system", "content": "You are a product data extraction expert. Return valid JSON only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=kwargs.get('temperature', self.temperature),
                max_tokens=kwargs.get('max_tokens', self.max_tokens)
            )
            
            content = response.choices[0].message.content.strip()
            self.logger.info(f"Azure OpenAI response received, length: {len(content)}")
            return content
            
        except Exception as e:
            self.logger.error(f"Azure OpenAI API error: {str(e)}")
            raise


class GoogleVertexAIClient(BaseLLMClient):
    """Google Vertex AI å®¢æˆ¶ç«¯"""
    
    def __init__(self, project_id: str, location: str, model_name: str = "text-bison", **kwargs):
        super().__init__("", model_name, **kwargs)  # Vertex AI ä½¿ç”¨æœå‹™å¸³æˆ¶èªè­‰
        self.project_id = project_id
        self.location = location
        
        # éœ€è¦å®‰è£ google-cloud-aiplatform
        try:
            from google.cloud import aiplatform
            aiplatform.init(project=project_id, location=location)
            self.aiplatform = aiplatform
        except ImportError:
            raise ImportError("Please install google-cloud-aiplatform: pip install google-cloud-aiplatform")
    
    async def complete(self, prompt: str, **kwargs) -> str:
        """ä½¿ç”¨ Google Vertex AI å®Œæˆæç¤º"""
        try:
            from google.cloud.aiplatform.gapic.schema import predict
            
            # å»ºæ§‹è«‹æ±‚
            instance = predict.instance.TextGenerationPredictInstance(
                prompt=prompt
            )
            
            parameters = predict.params.TextGenerationPredictParams(
                temperature=kwargs.get('temperature', 0.3),
                max_output_tokens=kwargs.get('max_tokens', 2000),
                top_p=kwargs.get('top_p', 0.8),
                top_k=kwargs.get('top_k', 40),
            )
            
            # å‘¼å« API
            endpoint = self.aiplatform.Endpoint(
                endpoint_name=f"projects/{self.project_id}/locations/{self.location}/endpoints/{self.model_name}"
            )
            
            response = await endpoint.predict_async(
                instances=[instance],
                parameters=parameters
            )
            
            content = response.predictions[0]['content'].strip()
            self.logger.info(f"Vertex AI response received, length: {len(content)}")
            return content
            
        except Exception as e:
            self.logger.error(f"Vertex AI error: {str(e)}")
            raise


# =============================================================================
# æ•´åˆåˆ° Echo-Roots çš„ç¯„ä¾‹
# =============================================================================

async def create_echo_roots_llm_extractor():
    """å»ºç«‹æ•´åˆäº†è‡ªè¨‚ LLM çš„ Echo-Roots æå–å™¨"""
    
    # 1. é¸æ“‡æ‚¨çš„ LLM å®¢æˆ¶ç«¯
    # é¸é … A: OpenAI
    llm_client = OpenAIClient(
        api_key="your-openai-api-key",
        model_name="gpt-4",
        temperature=0.3,
        max_tokens=2000
    )
    
    # é¸é … B: Anthropic Claude
    # llm_client = AnthropicClient(
    #     api_key="your-anthropic-api-key",
    #     model_name="claude-3-sonnet-20240229",
    #     max_tokens=2000
    # )
    
    # é¸é … C: Azure OpenAI
    # llm_client = AzureOpenAIClient(
    #     api_key="your-azure-api-key",
    #     endpoint="https://your-resource.openai.azure.com/",
    #     deployment_name="your-deployment-name",
    #     api_version="2024-02-15-preview"
    # )
    
    # é¸é … D: è‡ªè¨‚ API
    # llm_client = CustomAPIClient(
    #     api_key="your-custom-api-key",
    #     base_url="https://your-api.com",
    #     model_name="your-model-name"
    # )
    
    # 2. è¼‰å…¥é ˜åŸŸåŒ…
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent / "src"))
    
    from echo_roots.models.domain import DomainPack
    from echo_roots.pipelines.extraction import LLMExtractor, ExtractorConfig
    import yaml
    
    # è¼‰å…¥é›»å•†é ˜åŸŸåŒ…
    domain_path = Path("domains/ecommerce/domain.yaml")
    with open(domain_path, 'r', encoding='utf-8') as f:
        domain_config = yaml.safe_load(f)
    
    domain_pack = DomainPack(**domain_config)
    
    # 3. è¨­å®šæå–å™¨é…ç½®
    config = ExtractorConfig(
        model_name=llm_client.model_name,
        temperature=0.3,
        max_tokens=2000,
        enable_validation=True,
        retry_attempts=3,
        retry_delay=1.0
    )
    
    # 4. å»ºç«‹æå–å™¨
    extractor = LLMExtractor(
        domain_pack=domain_pack,
        llm_client=llm_client,
        config=config
    )
    
    return extractor


async def test_real_llm_extraction():
    """æ¸¬è©¦çœŸå¯¦ LLM æå–"""
    try:
        # å»ºç«‹æå–å™¨
        extractor = await create_echo_roots_llm_extractor()
        
        # è¼‰å…¥æ¸¬è©¦è³‡æ–™
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path(__file__).parent / "src"))
        
        from echo_roots.storage.duckdb_backend import DuckDBStorageManager
        
        storage_config = {
            "duckdb": {
                "database_path": "./test_echo_roots.db"
            }
        }
        
        storage_manager = DuckDBStorageManager(storage_config)
        await storage_manager.initialize()
        
        # ç²å–æ¸¬è©¦é …ç›®
        items = await storage_manager.ingestion.list_items(limit=1)
        if not items:
            print("âŒ No test data found. Please run test_data_processing.py first.")
            return
        
        # æ¸¬è©¦çœŸå¯¦ LLM æå–
        test_item = items[0]
        print(f"ğŸ¤– Testing real LLM extraction for: {test_item.title}")
        
        result = await extractor.extract_single(test_item)
        
        print(f"âœ… Extracted {len(result.attributes)} attributes:")
        for attr in result.attributes:
            print(f"  - {attr.name}: {attr.value} (confidence: {attr.confidence:.2f})")
            print(f"    Evidence: {attr.evidence}")
        
        print(f"âœ… Extracted {len(result.terms)} terms:")
        for term in result.terms[:5]:
            print(f"  - {term.term} (confidence: {term.confidence:.2f})")
        
        print(f"â±ï¸ Processing time: {result.metadata.processing_time_ms}ms")
        print(f"ğŸ”§ Model used: {result.metadata.model}")
        
        await storage_manager.close()
        
    except Exception as e:
        print(f"âŒ Error testing real LLM extraction: {str(e)}")
        import traceback
        traceback.print_exc()


# =============================================================================
# é…ç½®æª”æ¡ˆç¯„ä¾‹
# =============================================================================

def create_llm_config_file():
    """å»ºç«‹ LLM é…ç½®æª”æ¡ˆç¯„ä¾‹"""
    
    config = {
        "llm": {
            "provider": "openai",  # openai, anthropic, azure, custom, vertex
            "model_name": "gpt-4",
            "api_key": "${OPENAI_API_KEY}",  # ä½¿ç”¨ç’°å¢ƒè®Šæ•¸
            "temperature": 0.3,
            "max_tokens": 2000,
            "timeout": 30,
            
            # Azure å°ˆç”¨è¨­å®š
            "azure": {
                "endpoint": "${AZURE_OPENAI_ENDPOINT}",
                "deployment_name": "${AZURE_DEPLOYMENT_NAME}",
                "api_version": "2024-02-15-preview"
            },
            
            # è‡ªè¨‚ API è¨­å®š
            "custom": {
                "base_url": "${CUSTOM_API_URL}",
                "headers": {
                    "Authorization": "Bearer ${CUSTOM_API_KEY}",
                    "Custom-Header": "value"
                }
            },
            
            # Google Vertex AI è¨­å®š
            "vertex": {
                "project_id": "${GOOGLE_PROJECT_ID}",
                "location": "us-central1",
                "service_account_path": "${GOOGLE_SERVICE_ACCOUNT_JSON}"
            }
        },
        
        "extraction": {
            "enable_validation": True,
            "retry_attempts": 3,
            "retry_delay": 1.0,
            "batch_size": 10,
            "concurrent_requests": 5
        }
    }
    
    import yaml
    with open("llm_config.yaml", "w", encoding="utf-8") as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print("âœ… Created llm_config.yaml")
    print("ğŸ“ Please update the configuration with your actual API keys and settings")


if __name__ == "__main__":
    # å»ºç«‹é…ç½®æª”æ¡ˆç¯„ä¾‹
    create_llm_config_file()
    
    # æ¸¬è©¦çœŸå¯¦ LLM æå–ï¼ˆéœ€è¦å…ˆè¨­å®š API é‡‘é‘°ï¼‰
    # asyncio.run(test_real_llm_extraction())
