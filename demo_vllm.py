"""
vLLM Integration Demo for echo-roots.

This script demonstrates how to integrate vLLM with echo-roots extraction pipeline.
It provides examples for different deployment scenarios and use cases.
"""

import asyncio
import os
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from echo_roots.models.core import IngestionItem
from echo_roots.pipelines.llm_factory import LLMClientFactory


class VLLMDemo:
    """Demo class for vLLM integration."""
    
    def __init__(self):
        self.examples = []
    
    def add_example(self, name: str, description: str, config: dict, test_prompt: str = None):
        """Add a configuration example."""
        self.examples.append({
            "name": name,
            "description": description,
            "config": config,
            "test_prompt": test_prompt or "What is the capital of Taiwan? Answer in Chinese."
        })
    
    def setup_examples(self):
        """Setup various vLLM configuration examples."""
        
        # Local deployment with default settings
        self.add_example(
            "Local Default",
            "æœ¬åœ°éƒ¨ç½²ï¼Œä½¿ç”¨é è¨­è¨­å®š",
            {
                "vllm": {
                    "deployment_type": "local",
                    "model_name": "meta-llama/Llama-2-7b-chat-hf"
                }
            }
        )
        
        # Local deployment with custom settings
        self.add_example(
            "Local Custom",
            "æœ¬åœ°éƒ¨ç½²ï¼Œè‡ªå®šç¾©ç«¯å£å’Œåƒæ•¸",
            {
                "vllm": {
                    "deployment_type": "local",
                    "host": "localhost",
                    "port": 8001,
                    "model_name": "Qwen/Qwen-7B-Chat",
                    "timeout": 90
                }
            },
            "è«‹ç”¨ä¸­æ–‡å›ç­”ï¼šå°ç£çš„é¦–éƒ½æ˜¯å“ªè£¡ï¼Ÿ"
        )
        
        # Cloud deployment
        self.add_example(
            "Cloud Deployment",
            "é›²ç«¯éƒ¨ç½²ï¼Œä½¿ç”¨é ç¨‹ vLLM æœå‹™",
            {
                "vllm": {
                    "deployment_type": "cloud",
                    "base_url": "https://your-vllm-server.com/v1",
                    "model_name": "mistralai/Mixtral-8x7B-Instruct-v0.1",
                    "api_key": "your-api-key",
                    "timeout": 120
                }
            }
        )
        
        # Generic deployment
        self.add_example(
            "Generic",
            "é€šç”¨é…ç½®ï¼Œé©ç”¨æ–¼ä»»ä½• OpenAI ç›¸å®¹ç«¯é»",
            {
                "vllm": {
                    "base_url": "http://custom-host:9000/v1",
                    "model_name": "custom-model",
                    "api_key": "dummy-key"
                }
            }
        )
    
    def print_configuration_guide(self):
        """Print configuration guide."""
        print("ğŸ”§ vLLM Configuration Guide")
        print("=" * 50)
        
        print("\\nğŸ“‹ Available Configuration Examples:\\n")
        
        for i, example in enumerate(self.examples, 1):
            print(f"{i}. **{example['name']}**")
            print(f"   æè¿°: {example['description']}")
            print(f"   é…ç½®: {example['config']}")
            print(f"   æ¸¬è©¦æç¤º: {example['test_prompt']}")
            print()
    
    def print_server_setup_guide(self):
        """Print vLLM server setup guide."""
        print("\\nğŸš€ vLLM Server Setup Commands")
        print("=" * 40)
        
        setups = [
            {
                "name": "Llama-2 7B (è‹±æ–‡é€šç”¨)",
                "command": """python -m vllm.entrypoints.openai.api_server \\
    --model meta-llama/Llama-2-7b-chat-hf \\
    --port 8000 \\
    --host 0.0.0.0"""
            },
            {
                "name": "Qwen 7B (ä¸­æ–‡å„ªåŒ–)", 
                "command": """python -m vllm.entrypoints.openai.api_server \\
    --model Qwen/Qwen-7B-Chat \\
    --port 8001 \\
    --trust-remote-code"""
            },
            {
                "name": "Mixtral 8x7B (é«˜æ•ˆèƒ½)",
                "command": """python -m vllm.entrypoints.openai.api_server \\
    --model mistralai/Mixtral-8x7B-Instruct-v0.1 \\
    --port 8002 \\
    --tensor-parallel-size 2"""
            }
        ]
        
        for setup in setups:
            print(f"\\nğŸ“¦ {setup['name']}:")
            print(f"```bash")
            print(f"{setup['command']}")
            print(f"```")
    
    def print_environment_variables(self):
        """Print environment variable examples."""
        print("\\nğŸ” Environment Variables")
        print("=" * 35)
        
        envs = [
            ("VLLM_BASE_URL", "http://localhost:8000/v1", "vLLM æœå‹™å™¨ URL"),
            ("VLLM_MODEL_NAME", "meta-llama/Llama-2-7b-chat-hf", "æ¨¡å‹åç¨±"),
            ("VLLM_API_KEY", "dummy-key", "API é‡‘é‘° (æœ¬åœ°é€šå¸¸ä¸éœ€è¦)"),
            ("VLLM_DEPLOYMENT_TYPE", "local", "éƒ¨ç½²é¡å‹ (local/cloud)")
        ]
        
        print("\\nè¨­å®šç¯„ä¾‹:")
        for env_name, example_value, description in envs:
            print(f"export {env_name}='{example_value}'  # {description}")
    
    async def test_configuration_example(self, example_name: str):
        """Test a specific configuration example."""
        example = next((ex for ex in self.examples if ex["name"] == example_name), None)
        if not example:
            print(f"âŒ Example '{example_name}' not found")
            return False
        
        print(f"\\nğŸ§ª Testing: {example['name']}")
        print(f"ğŸ“ Description: {example['description']}")
        
        try:
            # Create client
            client = LLMClientFactory.create_client("vllm", example["config"])
            print(f"âœ… Client created: {type(client).__name__}")
            print(f"ğŸ”— URL: {getattr(client, 'base_url', 'N/A')}")
            print(f"ğŸ¤– Model: {getattr(client, 'model_name', 'N/A')}")
            
            # Test health check if available
            if hasattr(client, 'health_check'):
                try:
                    healthy = await asyncio.wait_for(client.health_check(), timeout=10)
                    if healthy:
                        print("âœ… Health check passed")
                        
                        # Test completion
                        response = await asyncio.wait_for(
                            client.complete(
                                example["test_prompt"],
                                max_tokens=100,
                                temperature=0.3
                            ),
                            timeout=30
                        )
                        print(f"âœ… Test completion successful")
                        print(f"ğŸ“„ Response: {response[:100]}...")
                        return True
                        
                    else:
                        print("âš ï¸  Health check failed - server not responding")
                        
                except asyncio.TimeoutError:
                    print("âš ï¸  Health check timeout - server may not be running")
                except Exception as e:
                    print(f"âš ï¸  Health check error: {e}")
            
            print("ğŸ’¡ Client created successfully but server not tested")
            return True
            
        except Exception as e:
            print(f"âŒ Configuration test failed: {e}")
            return False
    
    async def demo_extraction_pipeline(self):
        """Demonstrate extraction pipeline with vLLM."""
        print("\\n\\nğŸ”„ Extraction Pipeline Demo")
        print("=" * 40)
        
        # Try to use any available vLLM configuration
        test_configs = [
            ("Local Default", {"vllm": {"deployment_type": "local", "model_name": "meta-llama/Llama-2-7b-chat-hf"}}),
            ("Environment", {})  # Use environment variables
        ]
        
        working_client = None
        
        for config_name, config in test_configs:
            try:
                print(f"\\nğŸ”§ Trying {config_name} configuration...")
                client = LLMClientFactory.create_client("vllm", config)
                
                if hasattr(client, 'health_check'):
                    healthy = await asyncio.wait_for(client.health_check(), timeout=5)
                    if healthy:
                        working_client = client
                        print(f"âœ… Using {config_name} configuration")
                        break
                    else:
                        print(f"âš ï¸  {config_name} server not responding")
                else:
                    # Can't test health, but client was created
                    working_client = client
                    print(f"âš ï¸  {config_name} client created (server not tested)")
                    break
                    
            except Exception as e:
                print(f"âŒ {config_name} failed: {e}")
        
        if not working_client:
            print("\\nâŒ No working vLLM configuration found")
            print("ğŸ’¡ Make sure vLLM server is running and environment variables are set")
            return False
        
        # Create test items
        print("\\nğŸ“ Creating test items...")
        test_items = [
            IngestionItem(
                item_id="demo_001",
                title="Apple iPhone 15 Pro Max 1TB",
                description="æœ€æ–°çš„ iPhone æ——è‰¦æ©Ÿå‹ï¼Œæ­è¼‰ A17 Pro æ™¶ç‰‡ï¼Œéˆ¦é‡‘å±¬æ©Ÿèº«ï¼Œæ”¯æ´ USB-C",
                language="zh",
                source="vllm_demo"
            ),
            IngestionItem(
                item_id="demo_002", 
                title="Samsung Galaxy S24 Ultra 512GB",
                description="Samsung flagship smartphone with S Pen, 200MP camera, and AI features",
                language="en",
                source="vllm_demo"
            )
        ]
        
        print(f"âœ… Created {len(test_items)} test items")
        
        # Simple extraction test (without full domain configuration)
        print("\\nğŸ”¬ Testing simple extraction...")
        
        for item in test_items:
            try:
                prompt = f"""Extract product information from the following item:
Title: {item.title}
Description: {item.description}

Return JSON with extracted attributes in this format:
{{"brand": "extracted_brand", "product_type": "extracted_type", "model": "extracted_model"}}"""

                response = await asyncio.wait_for(
                    working_client.complete(prompt, max_tokens=200, temperature=0.1),
                    timeout=30
                )
                
                print(f"\\nğŸ“‹ Item: {item.title}")
                print(f"ğŸ¤– Extraction: {response[:150]}...")
                
            except Exception as e:
                print(f"âŒ Extraction failed for {item.item_id}: {e}")
        
        return True


async def main():
    """Main demo function."""
    print("ğŸš€ vLLM Integration Demo for echo-roots")
    print("=" * 60)
    
    demo = VLLMDemo()
    demo.setup_examples()
    
    # Show configuration guide
    demo.print_configuration_guide()
    demo.print_server_setup_guide() 
    demo.print_environment_variables()
    
    # Check current environment
    print("\\n\\nğŸ” Current Environment Status")
    print("=" * 40)
    
    vllm_vars = [
        ("VLLM_BASE_URL", os.getenv("VLLM_BASE_URL")),
        ("VLLM_MODEL_NAME", os.getenv("VLLM_MODEL_NAME")),
        ("VLLM_API_KEY", os.getenv("VLLM_API_KEY")),
        ("VLLM_DEPLOYMENT_TYPE", os.getenv("VLLM_DEPLOYMENT_TYPE"))
    ]
    
    for var_name, var_value in vllm_vars:
        if var_value:
            display_value = var_value if "API_KEY" not in var_name else var_value[:4] + "***" + var_value[-4:]
            print(f"âœ… {var_name}: {display_value}")
        else:
            print(f"âŒ {var_name}: Not set")
    
    # Provider availability check
    print("\\nğŸ”Œ Provider Availability Check")
    print("-" * 35)
    
    from echo_roots.pipelines.llm_factory import get_available_providers
    providers = get_available_providers()
    
    for provider, (available, status) in providers.items():
        symbol = "âœ…" if available else "âŒ"
        print(f"{symbol} {provider}: {status}")
    
    # Interactive testing
    print("\\n\\nğŸ¯ Interactive Testing")
    print("=" * 30)
    
    if len(sys.argv) > 1:
        example_name = sys.argv[1]
        print(f"Testing specific example: {example_name}")
        await demo.test_configuration_example(example_name)
    else:
        print("Available test examples:")
        for i, example in enumerate(demo.examples, 1):
            print(f"  {i}. {example['name']}")
        
        print("\\nğŸ’¡ Usage:")
        print(f"  python {sys.argv[0]} 'Local Default'")
        print(f"  python {sys.argv[0]} 'Local Custom'")
    
    # Try extraction demo
    await demo.demo_extraction_pipeline()
    
    print("\\n\\nğŸ“š Next Steps")
    print("=" * 20)
    print("1. å•Ÿå‹• vLLM ä¼ºæœå™¨ (åƒè€ƒä¸Šé¢çš„å‘½ä»¤)")
    print("2. è¨­å®šç’°å¢ƒè®Šæ•¸")
    print("3. åŸ·è¡Œå®Œæ•´æ¸¬è©¦: python test_vllm_integration.py")
    print("4. æŸ¥çœ‹æ–‡æª”: docs/guides/vllm-integration.md")


if __name__ == "__main__":
    asyncio.run(main())
