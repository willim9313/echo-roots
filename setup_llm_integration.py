#!/usr/bin/env python3
"""
å¯¦éš›æ•´åˆ LLM API åˆ° Echo-Roots çš„æ­¥é©ŸæŒ‡å—
Step-by-step guide to integrate your LLM API into Echo-Roots
"""

# ==============================================================================
# æ­¥é©Ÿ 1: åœ¨ç¾æœ‰çš„ extraction.py ä¸­æ·»åŠ æ‚¨çš„ LLM å®¢æˆ¶ç«¯
# ==============================================================================

"""
æ‚¨éœ€è¦åœ¨ src/echo_roots/pipelines/extraction.py ä¸­æ·»åŠ æ–°çš„ LLM å®¢æˆ¶ç«¯é¡åˆ¥ã€‚

æ‰¾åˆ° MockLLMClient é¡åˆ¥å¾Œé¢ï¼Œæ·»åŠ ä»¥ä¸‹ä»£ç¢¼ï¼š
"""

class YourCustomLLMClient:
    """æ‚¨çš„è‡ªè¨‚ LLM å®¢æˆ¶ç«¯"""
    
    def __init__(self, api_key: str, model_name: str = "gpt-4", **kwargs):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = kwargs.get('base_url', 'https://api.openai.com/v1')
        self.temperature = kwargs.get('temperature', 0.3)
        self.max_tokens = kwargs.get('max_tokens', 2000)
        
        # è¨­å®š HTTP å®¢æˆ¶ç«¯
        import aiohttp
        self.session = None
    
    async def complete(self, prompt: str, **kwargs) -> str:
        """å¯¦ç¾ LLMClient Protocol çš„ complete æ–¹æ³•"""
        import aiohttp
        import json
        
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'model': self.model_name,
            'messages': [
                {
                    'role': 'system', 
                    'content': 'You are a product data extraction expert. Return valid JSON only.'
                },
                {
                    'role': 'user', 
                    'content': prompt
                }
            ],
            'temperature': kwargs.get('temperature', self.temperature),
            'max_tokens': kwargs.get('max_tokens', self.max_tokens)
        }
        
        try:
            async with self.session.post(
                f'{self.base_url}/chat/completions',
                headers=headers,
                json=payload,
                timeout=30
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return data['choices'][0]['message']['content'].strip()
                else:
                    error_text = await response.text()
                    raise Exception(f"API Error {response.status}: {error_text}")
                    
        except Exception as e:
            raise Exception(f"LLM API call failed: {str(e)}")
    
    async def close(self):
        """é—œé–‰ HTTP æœƒè©±"""
        if self.session:
            await self.session.close()


# ==============================================================================
# æ­¥é©Ÿ 2: å‰µå»º LLM å®¢æˆ¶ç«¯å·¥å» å‡½æ•¸
# ==============================================================================

def create_llm_client(provider: str, **config) -> 'LLMClient':
    """æ ¹æ“šé…ç½®å‰µå»º LLM å®¢æˆ¶ç«¯"""
    
    if provider == "openai":
        return YourCustomLLMClient(
            api_key=config['api_key'],
            model_name=config.get('model_name', 'gpt-4'),
            base_url=config.get('base_url', 'https://api.openai.com/v1'),
            temperature=config.get('temperature', 0.3),
            max_tokens=config.get('max_tokens', 2000)
        )
    
    elif provider == "anthropic":
        # å¯¦ç¾ Anthropic å®¢æˆ¶ç«¯
        class AnthropicLLMClient:
            def __init__(self, api_key: str, model_name: str = "claude-3-sonnet-20240229", **kwargs):
                self.api_key = api_key
                self.model_name = model_name
                self.max_tokens = kwargs.get('max_tokens', 2000)
            
            async def complete(self, prompt: str, **kwargs) -> str:
                import aiohttp
                import json
                
                headers = {
                    'x-api-key': self.api_key,
                    'Content-Type': 'application/json',
                    'anthropic-version': '2023-06-01'
                }
                
                payload = {
                    'model': self.model_name,
                    'max_tokens': kwargs.get('max_tokens', self.max_tokens),
                    'messages': [
                        {
                            'role': 'user',
                            'content': f'{prompt}\n\nPlease respond with valid JSON only.'
                        }
                    ]
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        'https://api.anthropic.com/v1/messages',
                        headers=headers,
                        json=payload,
                        timeout=30
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            return data['content'][0]['text'].strip()
                        else:
                            error_text = await response.text()
                            raise Exception(f"Anthropic API Error {response.status}: {error_text}")
        
        return AnthropicLLMClient(
            api_key=config['api_key'],
            model_name=config.get('model_name', 'claude-3-sonnet-20240229'),
            max_tokens=config.get('max_tokens', 2000)
        )
    
    elif provider == "custom":
        # æ‚¨çš„è‡ªè¨‚ API å®¢æˆ¶ç«¯
        class CustomAPIClient:
            def __init__(self, **config):
                self.api_key = config['api_key']
                self.base_url = config['base_url']
                self.model_name = config['model_name']
                self.headers = config.get('headers', {})
            
            async def complete(self, prompt: str, **kwargs) -> str:
                import aiohttp
                
                headers = {
                    **self.headers,
                    'Authorization': f'Bearer {self.api_key}',
                    'Content-Type': 'application/json'
                }
                
                # æ ¹æ“šæ‚¨çš„ API æ ¼å¼èª¿æ•´ payload
                payload = {
                    'model': self.model_name,
                    'prompt': prompt,
                    'max_tokens': kwargs.get('max_tokens', 2000),
                    'temperature': kwargs.get('temperature', 0.3)
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f'{self.base_url}/completions',
                        headers=headers,
                        json=payload,
                        timeout=30
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            # æ ¹æ“šæ‚¨çš„ API éŸ¿æ‡‰æ ¼å¼èª¿æ•´
                            return data['choices'][0]['text'].strip()
                        else:
                            error_text = await response.text()
                            raise Exception(f"Custom API Error {response.status}: {error_text}")
        
        return CustomAPIClient(**config)
    
    else:
        raise ValueError(f"Unsupported LLM provider: {provider}")


# ==============================================================================
# æ­¥é©Ÿ 3: å‰µå»ºé…ç½®æ–‡ä»¶
# ==============================================================================

import yaml
import os
from pathlib import Path

def create_llm_config():
    """å‰µå»º LLM é…ç½®æ–‡ä»¶"""
    
    config = {
        'llm': {
            'provider': 'openai',  # æˆ– 'anthropic', 'custom'
            'api_key': os.getenv('OPENAI_API_KEY', 'your-api-key-here'),
            'model_name': 'gpt-4',
            'temperature': 0.3,
            'max_tokens': 2000,
            'base_url': 'https://api.openai.com/v1',  # å¯é¸ï¼Œç”¨æ–¼ API ä»£ç†
            
            # å¦‚æœä½¿ç”¨ Anthropic
            'anthropic': {
                'api_key': os.getenv('ANTHROPIC_API_KEY', 'your-anthropic-key'),
                'model_name': 'claude-3-sonnet-20240229'
            },
            
            # å¦‚æœä½¿ç”¨è‡ªè¨‚ API
            'custom': {
                'api_key': os.getenv('CUSTOM_API_KEY', 'your-custom-key'),
                'base_url': 'https://your-api.com/v1',
                'model_name': 'your-model-name',
                'headers': {
                    'Custom-Header': 'value'
                }
            }
        },
        
        'extraction': {
            'retry_attempts': 3,
            'retry_delay': 1.0,
            'timeout': 30,
            'enable_validation': True
        }
    }
    
    config_path = Path('config/llm_config.yaml')
    config_path.parent.mkdir(exist_ok=True)
    
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"âœ… å·²å‰µå»ºé…ç½®æ–‡ä»¶: {config_path}")
    return config_path


# ==============================================================================
# æ­¥é©Ÿ 4: ä¿®æ”¹ Echo-Roots ä»¥ä½¿ç”¨æ‚¨çš„ LLM
# ==============================================================================

def create_echo_roots_llm_integration():
    """å‰µå»ºæ•´åˆè…³æœ¬"""
    
    integration_code = '''
import asyncio
import yaml
from pathlib import Path
import sys

# æ·»åŠ  src åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent / "src"))

from echo_roots.models.domain import DomainPack
from echo_roots.pipelines.extraction import LLMExtractor, ExtractorConfig
from echo_roots.storage.duckdb_backend import DuckDBStorageManager

# å°å…¥æ‚¨çš„ LLM å®¢æˆ¶ç«¯å‰µå»ºå‡½æ•¸
from llm_api_integration import create_llm_client

async def run_extraction_with_your_llm():
    """ä½¿ç”¨æ‚¨çš„ LLM API é‹è¡Œæå–"""
    
    # 1. è¼‰å…¥é…ç½®
    with open('config/llm_config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # 2. å‰µå»º LLM å®¢æˆ¶ç«¯
    llm_config = config['llm']
    llm_client = create_llm_client(
        provider=llm_config['provider'],
        **llm_config
    )
    
    # 3. è¼‰å…¥é ˜åŸŸåŒ…
    domain_path = Path("domains/ecommerce/domain.yaml")
    with open(domain_path, 'r', encoding='utf-8') as f:
        domain_config = yaml.safe_load(f)
    
    domain_pack = DomainPack(**domain_config)
    
    # 4. å‰µå»ºæå–å™¨
    extractor_config = ExtractorConfig(
        model_name=llm_config['model_name'],
        temperature=llm_config['temperature'],
        max_tokens=llm_config['max_tokens'],
        retry_attempts=config['extraction']['retry_attempts'],
        retry_delay=config['extraction']['retry_delay'],
        enable_validation=config['extraction']['enable_validation']
    )
    
    extractor = LLMExtractor(
        domain_pack=domain_pack,
        llm_client=llm_client,
        config=extractor_config
    )
    
    # 5. è¼‰å…¥æ¸¬è©¦è³‡æ–™
    storage_config = {
        "duckdb": {
            "database_path": "./test_echo_roots.db"
        }
    }
    
    storage_manager = DuckDBStorageManager(storage_config)
    await storage_manager.initialize()
    
    # 6. ç²å–è¦è™•ç†çš„é …ç›®
    items = await storage_manager.ingestion.list_items(limit=3)
    
    if not items:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æ¸¬è©¦è³‡æ–™ã€‚è«‹å…ˆé‹è¡Œ test_data_processing.py")
        return
    
    # 7. é‹è¡Œæå–
    print(f"ğŸ¤– ä½¿ç”¨ {llm_config['provider']} {llm_config['model_name']} é€²è¡Œæå–...")
    
    for i, item in enumerate(items, 1):
        print(f"\\nğŸ“ è™•ç†é …ç›® {i}: {item.title}")
        
        try:
            result = await extractor.extract_single(item)
            
            print(f"âœ… æå–äº† {len(result.attributes)} å€‹å±¬æ€§:")
            for attr in result.attributes:
                print(f"  - {attr.name}: {attr.value} (ä¿¡å¿ƒåº¦: {attr.confidence:.2f})")
            
            print(f"âœ… æå–äº† {len(result.terms)} å€‹è¡“èª:")
            for term in result.terms[:3]:
                print(f"  - {term.term} (ä¿¡å¿ƒåº¦: {term.confidence:.2f})")
            
            print(f"â±ï¸ è™•ç†æ™‚é–“: {result.metadata.processing_time_ms}ms")
            
        except Exception as e:
            print(f"âŒ æå–å¤±æ•—: {str(e)}")
    
    # 8. æ¸…ç†
    await storage_manager.close()
    
    # å¦‚æœ LLM å®¢æˆ¶ç«¯æœ‰æ¸…ç†æ–¹æ³•ï¼Œèª¿ç”¨å®ƒ
    if hasattr(llm_client, 'close'):
        await llm_client.close()
    
    print("\\nğŸ‰ æå–å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(run_extraction_with_your_llm())
'''
    
    with open('run_with_your_llm.py', 'w', encoding='utf-8') as f:
        f.write(integration_code)
    
    print("âœ… å·²å‰µå»º run_with_your_llm.py")


# ==============================================================================
# æ­¥é©Ÿ 5: ç’°å¢ƒè®Šæ•¸è¨­å®šç¯„ä¾‹
# ==============================================================================

def create_env_example():
    """å‰µå»ºç’°å¢ƒè®Šæ•¸ç¯„ä¾‹æ–‡ä»¶"""
    
    env_content = '''# LLM API é…ç½®
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
CUSTOM_API_KEY=your_custom_api_key_here

# Azure OpenAI (å¦‚æœä½¿ç”¨)
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_DEPLOYMENT_NAME=your-deployment-name
AZURE_API_KEY=your-azure-api-key

# Google Cloud (å¦‚æœä½¿ç”¨)
GOOGLE_PROJECT_ID=your-project-id
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

# è‡ªè¨‚è¨­å®š
CUSTOM_API_URL=https://your-api.com
CUSTOM_MODEL_NAME=your-model-name
'''
    
    with open('.env.example', 'w', encoding='utf-8') as f:
        f.write(env_content)
    
    print("âœ… å·²å‰µå»º .env.example")
    print("ğŸ“ è«‹è¤‡è£½åˆ° .env ä¸¦å¡«å…¥æ‚¨çš„å¯¦éš› API é‡‘é‘°")


# ==============================================================================
# ä¸»è¦å‡½æ•¸
# ==============================================================================

def main():
    """è¨­ç½® LLM API æ•´åˆ"""
    
    print("ğŸ”§ è¨­ç½® Echo-Roots LLM API æ•´åˆ...")
    
    # å‰µå»ºå¿…è¦çš„æ–‡ä»¶
    config_path = create_llm_config()
    create_echo_roots_llm_integration()
    create_env_example()
    
    print(f"""
ğŸ‰ LLM API æ•´åˆè¨­ç½®å®Œæˆï¼

ä¸‹ä¸€æ­¥:
1. ç·¨è¼¯ {config_path} è¨­ç½®æ‚¨çš„ LLM æä¾›å•†å’Œæ¨¡å‹
2. è¤‡è£½ .env.example åˆ° .env ä¸¦å¡«å…¥ API é‡‘é‘°
3. é‹è¡Œ: python run_with_your_llm.py

æ”¯æ´çš„ LLM æä¾›å•†:
- OpenAI (gpt-4, gpt-3.5-turbo)
- Anthropic (claude-3-sonnet, claude-3-haiku)
- Azure OpenAI
- è‡ªè¨‚ API

éœ€è¦å®‰è£çš„å¥—ä»¶:
pip install aiohttp
pip install openai  # å¦‚æœä½¿ç”¨ OpenAI
pip install anthropic  # å¦‚æœä½¿ç”¨ Anthropic
""")


if __name__ == "__main__":
    main()
