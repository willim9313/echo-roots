#!/usr/bin/env python3
"""
LLM æå–æ¸¬è©¦è…³æœ¬
Test script for LLM extraction functionality
"""

import asyncio
import json
import yaml
from pathlib import Path
from typing import Dict, Any, List
import sys

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from echo_roots.models.core import IngestionItem, ExtractionResult, AttributeExtraction, SemanticTerm, ExtractionMetadata
from echo_roots.models.domain import DomainPack
from echo_roots.pipelines.extraction import LLMExtractor, ExtractorConfig
from echo_roots.storage.duckdb_backend import DuckDBStorageManager


async def load_test_data():
    """è¼‰å…¥æ¸¬è©¦è³‡æ–™"""
    print("ğŸ“„ Loading test data from storage...")
    
    # é…ç½®å„²å­˜
    storage_config = {
        "duckdb": {
            "database_path": "./test_echo_roots.db"
        }
    }
    
    storage_manager = DuckDBStorageManager(storage_config)
    await storage_manager.initialize()
    
    # ç²å–å„²å­˜çš„æ¸¬è©¦è³‡æ–™
    ingestion_repo = storage_manager.ingestion
    items = await ingestion_repo.list_items(limit=5)
    
    print(f"âœ… Loaded {len(items)} items from storage")
    for i, item in enumerate(items, 1):
        print(f"  {i}. {item.title} ({item.raw_category})")
    
    await storage_manager.close()
    return items


async def test_mock_llm_extraction(items: List[IngestionItem]):
    """æ¸¬è©¦æ¨¡æ“¬ LLM æå–"""
    print("\nğŸ¤– Testing Mock LLM Extraction...")
    
    # è¼‰å…¥é ˜åŸŸåŒ…é…ç½®
    domain_path = Path("domains/ecommerce")
    domain_file = domain_path / "domain.yaml"
    
    with open(domain_file, 'r', encoding='utf-8') as f:
        domain_config = yaml.safe_load(f)
    
    domain_pack = DomainPack(**domain_config)
    
    # ç°¡åŒ–çš„æ¨¡æ“¬æå–å™¨
    async def mock_extract_attributes(item: IngestionItem) -> ExtractionResult:
        """æ¨¡æ“¬å±¬æ€§æå–"""
        
        # æ ¹æ“šå•†å“é¡å‹æ¨¡æ“¬æå–ä¸åŒå±¬æ€§
        mock_attributes = []
        mock_terms = []
        
        # åˆ†ææ¨™é¡Œå’Œæè¿°ï¼Œæ¨¡æ“¬ LLM ç†è§£
        title_lower = item.title.lower()
        description_lower = (item.description or "").lower()
        
        # å“ç‰Œæå–
        brand_keywords = {
            'apple': 'Apple',
            'iphone': 'Apple', 
            'samsung': 'Samsung',
            'galaxy': 'Samsung',
            'lenovo': 'Lenovo',
            'thinkpad': 'Lenovo',
            'sony': 'Sony',
            'nintendo': 'Nintendo',
            'asus': 'ASUS',
            'xiaomi': 'Xiaomi',
            'å°ç±³': 'Xiaomi'
        }
        
        for keyword, brand in brand_keywords.items():
            if keyword in title_lower or keyword in description_lower:
                mock_attributes.append(AttributeExtraction(
                    name="brand",
                    value=brand,
                    evidence=f"Found '{keyword}' in title",
                    confidence=0.95
                ))
                break
        
        # é¡è‰²æå–
        color_keywords = {
            'é»‘è‰²': 'black', 'ç™½è‰²': 'white', 'éŠ€è‰²': 'silver',
            'é‡‘è‰²': 'gold', 'è—è‰²': 'blue', 'ç´…è‰²': 'red',
            'ç°è‰²': 'gray', 'éˆ¦é‡‘å±¬': 'titanium', 'å¤©ç„¶éˆ¦é‡‘å±¬': 'natural titanium',
            'éˆ¦ç°è‰²': 'titanium gray'
        }
        
        for color_cn, color_en in color_keywords.items():
            if color_cn in item.title or color_cn in (item.description or ""):
                mock_attributes.append(AttributeExtraction(
                    name="color",
                    value=color_en,
                    evidence=f"Found color '{color_cn}' in text",
                    confidence=0.90
                ))
                break
        
        # å®¹é‡/å°ºå¯¸æå–
        import re
        
        # å„²å­˜å®¹é‡
        storage_match = re.search(r'(\d+)GB', item.title)
        if storage_match:
            capacity = storage_match.group(1) + "GB"
            mock_attributes.append(AttributeExtraction(
                name="storage_capacity",
                value=capacity,
                evidence=f"Extracted storage: {capacity}",
                confidence=0.98
            ))
        
        # è¢å¹•å°ºå¯¸
        screen_match = re.search(r'(\d+\.?\d*)å‹', item.title + " " + (item.description or ""))
        if screen_match:
            size = screen_match.group(1) + "å‹"
            mock_attributes.append(AttributeExtraction(
                name="screen_size",
                value=size,
                evidence=f"Extracted screen size: {size}",
                confidence=0.90
            ))
        
        # åƒ¹æ ¼ç¯„åœï¼ˆåŸºæ–¼å¯¦éš›åƒ¹æ ¼ï¼‰
        if 'price' in item.raw_attributes:
            price = item.raw_attributes['price']
            if price < 1000:
                price_range = "budget"
            elif price < 10000:
                price_range = "mid-range"
            else:
                price_range = "premium"
            
            mock_attributes.append(AttributeExtraction(
                name="price_range",
                value=price_range,
                evidence=f"Price {price} categorized as {price_range}",
                confidence=0.85
            ))
        
        # è¡“èªæå–ï¼ˆé—œéµè©ï¼‰
        tech_terms = [
            'A17 Pro', 'AMOLED', 'OLED', 'Retina', 'Dynamic', 
            'ä¸»å‹•å¼é™å™ª', 'è—ç‰™', 'WiFi', 'USB-C', 'å¿«å……',
            'é˜²æ°´', 'æŒ‡ç´‹', 'Face ID', 'NFC'
        ]
        
        text_content = item.title + " " + (item.description or "")
        for term in tech_terms:
            if term in text_content:
                mock_terms.append(SemanticTerm(
                    term=term,
                    context=f"Found in product description",
                    confidence=0.80
                ))
        
        # å‰µå»ºæå–çµæœ
        result = ExtractionResult(
            item_id=item.item_id,
            attributes=mock_attributes,
            terms=mock_terms,
            metadata=ExtractionMetadata(
                model="mock-llm-v1",
                run_id=f"mock-run-{hash(item.item_id)}",
                extracted_at="2025-09-08T22:45:00Z",
                processing_time_ms=150  # æ¨¡æ“¬è™•ç†æ™‚é–“
            )
        )
        
        return result
    
    # æ¸¬è©¦æå–
    for i, item in enumerate(items[:3], 1):
        print(f"\nğŸ” Processing item {i}: {item.title}")
        
        try:
            result = await mock_extract_attributes(item)
            
            print(f"âœ… Extracted {len(result.attributes)} attributes:")
            for attr in result.attributes:
                print(f"  - {attr.name}: {attr.value} (confidence: {attr.confidence:.2f})")
                print(f"    Evidence: {attr.evidence}")
            
            print(f"âœ… Extracted {len(result.terms)} terms:")
            for term in result.terms[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                print(f"  - {term.term} (confidence: {term.confidence:.2f})")
            
            print(f"â±ï¸ Processing time: {result.metadata.processing_time_ms}ms")
            
        except Exception as e:
            print(f"âŒ Failed to extract from item {i}: {str(e)}")


async def test_category_mapping(items: List[IngestionItem]):
    """æ¸¬è©¦åˆ†é¡æ˜ å°„"""
    print("\nğŸ·ï¸ Testing Category Mapping...")
    
    # è¼‰å…¥åˆ†é¡çµæ§‹
    taxonomy_path = Path("domains/ecommerce/categories/taxonomy.yaml")
    with open(taxonomy_path, 'r', encoding='utf-8') as f:
        taxonomy_config = yaml.safe_load(f)
    
    categories = taxonomy_config['categories']
    
    # å»ºç«‹åˆ†é¡æ˜ å°„å­—å…¸
    category_map = {}
    
    def build_category_paths(cats, parent_path=""):
        for cat in cats:
            current_path = f"{parent_path}/{cat['name']}" if parent_path else cat['name']
            category_map[cat['name']] = {
                'id': cat['id'],
                'path': current_path,
                'level': cat['level']
            }
            
            if 'children' in cat:
                build_category_paths(cat['children'], current_path)
    
    build_category_paths(categories)
    
    print(f"ğŸ“š Loaded {len(category_map)} categories from taxonomy")
    
    # æ¸¬è©¦æ˜ å°„
    mapping_results = []
    
    for item in items[:5]:
        print(f"\nğŸ“ Mapping: {item.title}")
        print(f"   Raw category: {item.raw_category}")
        
        # ç°¡å–®çš„é—œéµè©åŒ¹é…
        best_match = None
        best_score = 0
        
        if item.raw_category:
            raw_parts = item.raw_category.replace('>', '/').split('/')
            
            for cat_name, cat_info in category_map.items():
                # è¨ˆç®—åŒ¹é…åˆ†æ•¸
                score = 0
                for part in raw_parts:
                    if part.strip() in cat_name or cat_name in part.strip():
                        score += 1
                
                if score > best_score:
                    best_score = score
                    best_match = cat_info
        
        if best_match:
            print(f"   âœ… Mapped to: {best_match['path']} (score: {best_score})")
            mapping_results.append({
                'item_id': item.item_id,
                'raw_category': item.raw_category,
                'mapped_category': best_match['path'],
                'confidence': best_score / len(item.raw_category.split('/')) if item.raw_category else 0
            })
        else:
            print(f"   âŒ No mapping found")
            mapping_results.append({
                'item_id': item.item_id,
                'raw_category': item.raw_category,
                'mapped_category': None,
                'confidence': 0
            })
    
    # é¡¯ç¤ºæ˜ å°„çµ±è¨ˆ
    mapped_count = sum(1 for r in mapping_results if r['mapped_category'])
    avg_confidence = sum(r['confidence'] for r in mapping_results) / len(mapping_results)
    
    print(f"\nğŸ“Š Mapping Statistics:")
    print(f"   Mapped: {mapped_count}/{len(mapping_results)} ({mapped_count/len(mapping_results)*100:.1f}%)")
    print(f"   Average confidence: {avg_confidence:.2f}")


async def test_data_quality_analysis(items: List[IngestionItem]):
    """æ¸¬è©¦è³‡æ–™å“è³ªåˆ†æ"""
    print("\nğŸ“Š Testing Data Quality Analysis...")
    
    quality_metrics = {
        'total_items': len(items),
        'with_description': 0,
        'with_category': 0,
        'with_attributes': 0,
        'avg_title_length': 0,
        'avg_description_length': 0,
        'unique_sources': set(),
        'languages': set(),
        'attribute_coverage': {}
    }
    
    total_title_length = 0
    total_desc_length = 0
    
    for item in items:
        # åŸºæœ¬å®Œæ•´æ€§
        if item.description:
            quality_metrics['with_description'] += 1
            total_desc_length += len(item.description)
        
        if item.raw_category:
            quality_metrics['with_category'] += 1
        
        if item.raw_attributes:
            quality_metrics['with_attributes'] += 1
        
        total_title_length += len(item.title)
        
        # ä¾†æºå’Œèªè¨€
        quality_metrics['unique_sources'].add(item.source)
        quality_metrics['languages'].add(item.language)
        
        # å±¬æ€§è¦†è“‹ç‡
        for attr_key in item.raw_attributes.keys():
            if attr_key not in quality_metrics['attribute_coverage']:
                quality_metrics['attribute_coverage'][attr_key] = 0
            quality_metrics['attribute_coverage'][attr_key] += 1
    
    # è¨ˆç®—å¹³å‡å€¼
    quality_metrics['avg_title_length'] = total_title_length / len(items)
    quality_metrics['avg_description_length'] = total_desc_length / quality_metrics['with_description'] if quality_metrics['with_description'] > 0 else 0
    
    # é¡¯ç¤ºçµæœ
    print(f"ğŸ“ˆ Data Quality Metrics:")
    print(f"   Total items: {quality_metrics['total_items']}")
    print(f"   With description: {quality_metrics['with_description']} ({quality_metrics['with_description']/len(items)*100:.1f}%)")
    print(f"   With category: {quality_metrics['with_category']} ({quality_metrics['with_category']/len(items)*100:.1f}%)")
    print(f"   With attributes: {quality_metrics['with_attributes']} ({quality_metrics['with_attributes']/len(items)*100:.1f}%)")
    print(f"   Avg title length: {quality_metrics['avg_title_length']:.1f} chars")
    print(f"   Avg description length: {quality_metrics['avg_description_length']:.1f} chars")
    print(f"   Unique sources: {len(quality_metrics['unique_sources'])}")
    print(f"   Languages: {list(quality_metrics['languages'])}")
    
    print(f"\nğŸ·ï¸ Top attributes:")
    sorted_attrs = sorted(quality_metrics['attribute_coverage'].items(), key=lambda x: x[1], reverse=True)
    for attr, count in sorted_attrs[:8]:
        coverage = count / len(items) * 100
        print(f"   {attr}: {count} items ({coverage:.1f}%)")


async def main():
    """ä¸»è¦æ¸¬è©¦æµç¨‹"""
    print("ğŸ¤– Echo-Roots LLM Extraction Test")
    print("=" * 50)
    
    try:
        # 1. è¼‰å…¥æ¸¬è©¦è³‡æ–™
        items = await load_test_data()
        if not items:
            print("âŒ No test data found. Please run test_data_processing.py first.")
            return
        
        # 2. æ¸¬è©¦æ¨¡æ“¬ LLM æå–
        await test_mock_llm_extraction(items)
        
        # 3. æ¸¬è©¦åˆ†é¡æ˜ å°„
        await test_category_mapping(items)
        
        # 4. æ¸¬è©¦è³‡æ–™å“è³ªåˆ†æ
        await test_data_quality_analysis(items)
        
        print("\nğŸ‰ LLM extraction tests completed successfully!")
        print("\nNext steps:")
        print("1. Integrate real LLM provider (OpenAI, Anthropic, etc.)")
        print("2. Fine-tune extraction prompts")
        print("3. Test semantic enrichment pipeline")
        print("4. Validate taxonomy mapping accuracy")
        
    except Exception as e:
        print(f"âŒ Test failed with error: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
